---
title: "rdhs overview"
author: "OJ Watson"
date: '`r Sys.Date()`'
output: rmarkdown::html_vignette
fig_caption: yes
theme: readable
toc: yes
vignette: >
  %\VignetteIndexEntry{Overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
NOT_CRAN <- identical(tolower(Sys.getenv("NOT_CRAN")), "true")
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Overview

The rdhs

## rdhs client

rdhs was built around an api client that would handle making and parsing api requests
to the DHS api. From that, a more nuanced general client tool was created that both made
api requests, ensuring that results are cached so they are not looked up again, but also
handled the downstream functionality of rdhs. As a result the best way to demnstrate rdhs
is to consider the current pipeline through which you might first use rdhs. Each of the following
steps are all carried out using the rdhs client, which is an R6 class (similar to R's built in
reference classes and make caching survey and api queries more reproducible):

1. Create the rdhs client
2. Making api requests
3. Using api requests to identify surveys you want to download
4. Cataloguing which surveys you can download for your log in credentials
5. Downloading the desired surveys from step 3 from the available surveys from step 4
6. Creating the list of survey questions required for your investigation
7. Using the list from step 6 to create your data frame of survey responses
8. Storing your pipeline from step 1 - 7 to increase reproducibility

Before demonstrating each section, it is worth highlighting why all the above steps are
carried out using the highly object-orientated client. The client itself caches the results
from each of the above step locally in one way or another. For example, the client when first
created is saved in a new local directory that you have specified. Once created it stores the 
time at which it was last updated, so that everytime you use rdhs afterwards, the client will
check to see if any of the surveys you have downloaded, or api calls you have made are now out
of date. If this happens, the client will let you know, before giving you options over whether
to a) compleletely delete the client's cache, or b) update any cached surveys or information
in bulk now or c) as and when you itneract with out of date information. In this way, 
rdhs hopes to allow studies and analayis of DHS data to both be easier, and reproducible allowing
researchers to save their analysis pipeline so that others can replicate it easily.

---

## 0. Installing the package

```{r Installing the package, include=TRUE, message = FALSE, warning = FALSE}

# First make sure the package is installed
devtools::install_github("OJWatson/rdhs")

# Load package - Warning messages will be thrown however these are not a problem
library(rdhs)
```

## 1. Create the rdhs client

The client is the bulk of the package's functionality, and at the moment there are
3 named arguments, *api_key*, *root* and *credentials*. You can apply for an API key from the DHS
website that will allow you to leverage more api call returns, and will be stroed within
the client if declared here. The root argument is the directory path where the client
and associated caches will be stored. If left bank, a suitable directory will be created
within your user cahce directory for your operating system. E.g:

* Mac OS X: ‘~/Library/Caches/<AppName>’
* Unix: ‘~/.cache/<AppName>’, $XDG_CACHE_HOME if defined

The credential argument is your login information for the DHS website. THis should take the form
of a file path that contains 3 lines, something like this:

email="dummy@gmail.com"
password="dummypass"
project="Dummy Project"

These will then be read in and used to create system environment variables so you won't have to 
re-enter them within the same session. 

```{r Client creation, include=TRUE, message = FALSE, warning = FALSE, purl = NOT_CRAN}

# Create a client withi our api key and in this root directory
root <- file.path(getwd(),"rt")
client <- rdhs::dhs_client(api_key = "ICLSPH-527168",root = root,credentials=".credentials")

# Have a look at what the client object is
client

```

The root directory you chose will now also have in it an .rds file that is your dhs client, 
and a series of subfolders that are used to cache data. In order to use this cached data when 
you start your next R session you would use the same code as above, but this time it will 
know you have already created a client and will load it instead. 

## 2. Making api requests

The client can then be used to conduct api requests. To find more information about the
possible api requests head to their [website](https://api.dhsprogram.com/#/index.html).
In brief the api allows for a number of different endpoints to be accessed that constitute one of
12 different databases. These are stored within the client by default, and are the following:

* [data](https://api.dhsprogram.com/#/api-data.cfm)
* [indicators](https://api.dhsprogram.com/#/api-indicators.cfm)
* [countries](https://api.dhsprogram.com/#/api-countries.cfm)
* [surveys](https://api.dhsprogram.com/#/api-surveys.cfm)
* [surveycharacteristics](https://api.dhsprogram.com/#/api-surveycharacteristics.cfm)
* [publications](https://api.dhsprogram.com/#/api-publications.cfm)
* [datasets](https://api.dhsprogram.com/#/api-datasets.cfm)
* [geometry](https://api.dhsprogram.com/#/api-geometry.cfm)
* [tags](https://api.dhsprogram.com/#/api-tags.cfm)
* [dataupdates](https://api.dhsprogram.com/#/api-dataupdates.cfm)
* [uiupdates](https://api.dhsprogram.com/#/api-uiupdates.cfm)
* [info](https://api.dhsprogram.com/#/api-info.cfm)

These endpoints can then be further filtered, with a different set of filters possible for 
each endpoint. At current, these are not built in so it is possible to make human errors when
submitting filter queries. To find info about all the possible filter queries head to the associated
webpage from above.

For example we can now query to find out the trends in antimalarial use in Africa, and see if 
perhaps antimalarial perscription has decreased after RDTs were introduced (assumed 2010). So we can query 
for that by specifying the "data" endpoint, and using suitable filter queries provided as a list as follows:


```{r API call, include=TRUE, message = FALSE, warning = FALSE, fig.width=10, fig.height=8}

# Make an api request
resp <- client$dhs_api_request(api_endpoint = "data",
                       query = list("indicatorIds"="ML_FEVT_C_AML",
                                    "surveyYearStart"=2006,
                                    "breakdown"="subnational"))


# and let's filter it by some countries
countries  <- c("Angola","Ghana","Kenya","Liberia",
                "Madagascar","Mali","Malawi","Nigeria",
                "Rwanda","Sierra Leone","Senegal","Tanzania","Uganda")

library(ggplot2)
ggplot(resp[resp$CountryName %in% countries,], aes(x=SurveyYear,y=Value,colour=CountryName)) +
  geom_point() + geom_smooth(method = "glm") + ylab(resp$Indicator[1]) + facet_wrap(~CountryName)

```

If we miss entered a filter query (very possible) rdhs will give you an error that is
somewhat nicely presented on the pagee for you to see way:

```{r API fail, include=TRUE, message = FALSE, error=TRUE, purl= FALSE, warning = FALSE}

# Make an api request
resp <- client$dhs_api_request(api_endpoint = "data",
                       query = list("indicatorIds"="ML_FEVT_C_AMasfafasfL",
                                    "surveyYearStart"=202231231306,
                                    "breakdownAndCry"="subParTyping"))

```


rdhs parses the response for you and returns a useful list where you can check that the
api call has been handled as expected. We can also then look at the raw data within this list.
If we do this we will see that only the first 100 results have been returned as this is the 
default page size return. If, however, you want all the results (most likely) then use the 
**all_results** argument within dhs_api_rquest. This will work by making an initial request,
assessing the number of pages it would require, and then either making the request again for 
all the results if less than your api_key will allow (5000), or by looping with page sizes
of 5000 through each page. (Still To be written)

Importantly the api request you have made has now been cached by the package. This means that
the next time you run this request, the result both returns quicker and you can access your data
even if not connected to the internet.

What if the DHS api is updated? Will my cached api calls now be wrong? Potentially. It is unclear
from the api how much information changes over time. The [dataupdates](https://api.dhsprogram.com/rest/dhs/dataupdates?f=html) 
endpoint shows updates to specific surveys. However, when looking at the [datasets](https://api.dhsprogram.com/rest/dhs/datasets?f=html) endpoint we can see that there are many survey files that have been presumably modified after their release. 
This may be just reuploading, or it could have changed the data and thus subsequent date we may get from api calls. 
As such, at this moment in time, if the date at which any of the DHS api was last updated as found
within the dataupdates endpoint is more recent than the cache_date within our client we will automatically
clear our api call cache. The api calls are relatively lightweight, so will not be a major hindrance
in the future. This can be seen below:

```{r API dating, include=TRUE, message = FALSE, warning = FALSE}

# Get the current cache date that the client was last created at
client$get_cache_date()

# Now let's set it back in time a year
client$set_cache_date(client$get_cache_date()-(365*24*60*60))
client$get_cache_date()

# And save this client 
client$save_client()

# Now let's imagine we were restarting our R session another time and wanted to load our client we were using before
client <- rdhs::dhs_client(api_key = "ICLSPH-527168",
                           root = root, 
                           credentials = ".credentials")
client$get_cache_date()

```

## 3. Using api requests to identify surveys you want to download

Working with the api can allow you to get good access to course level data, however you may want access
to the raw datasets to compile individual level data. To do this you will need to decide what surveys you 
want for your survey and then download them. This used to be laborious... and is somewhat nice now.

First we will have a look using the api what surveys record data about children under age five with fever 
in the two weeks preceding the survey that received any anti-malarial drugs who took Quinine. 

```{r API for surveys, include=TRUE, message = FALSE, warning = FALSE}

# Make an api request
desired_data <- client$dhs_api_request(api_endpoint = "surveys",
                       query = list("indicatorIds"="ML_AMLD_C_QNN"))

str(desired_data)

```

Now we have a data frame for the surveys we want we then go about actually downloading
the surveys.

## 4. Cataloguing which surveys you can download for your log in credentials

To do this we again use the client, and pass in our login in credentials, and where we will
eventually want to download our surveys too. The client will then log into your DHS account
for you and scrape what surveys you have access to. It will then turn these into downloadable
urls to be used in the next step. (Takes ~30s)


```{r Avaialable surveys, include=TRUE, message = FALSE, warning = FALSE}

# client will fetch all the surveys that you can download from the DHS 
# website (i.e. the ones they have gien you permission for)
available_surveys <- client$available_surveys()

str(available_surveys)
```


## 5. Downloading the desired surveys from step 3 from the available surveys from step 4

Now that we have the list of surveys, and we now what ones we want to actually download, we simply
subset those from step 4 using our dataframe in step 3.

```{r Subset surveys, include=TRUE, message = FALSE, warning = FALSE}

# subset our available data frames
subsetted_surveys <- dplyr::filter(available_surveys, SurveyId %in% desired_data$SurveyId)

```

And we can now download those surveys using our client.

N.B It is recommended (and there is only support for) to download the Stata datasets and the SPSS datasets,
and preferentially the Stata datasets as these are a little quicker to import. This obviously stands for just the 
survey datasets, whereas the GPS coordinates (which are logged as .dat files) are supported as they are 
read in using \code{rgdal::readOGR}.

The default download will also 

```{r Download surveys, include=TRUE, message = FALSE, warning = FALSE, collapse = TRUE, comment = "#>"}

# let's then grab the stata datasets
stata_surveys <- subsetted_surveys[subsetted_surveys$FileFormat=="Stata dataset (.dta)",]

# download datasets
downloads <- client$download_survey(desired_survey=stata_surveys[c(1,25,50,75,100),], reformat = TRUE)

```

At the moment this will indisciriminately download the zip file and unpack it to a directory within
the directory path we specified as the output_dir argument for `available_surveys`. So this looks something like this:

```{r Download tree, include=TRUE, message = FALSE, warning = FALSE}

library(data.tree)
x <- lapply(strsplit(list.files(file.path(root,"surveys"),recursive = T,full.names = TRUE), "/"),
            function(z) as.data.frame(t(z)))
x <- dplyr::rbind_all(x)
x$pathString <- apply(x, 1, function(x) paste(trimws(na.omit(tail(x,4))), collapse="/"))
mytree <- data.tree::as.Node(x)
mytree

```

Where you can see that a new directory is made for each survey, with any datasets collected from that
survey, e.g. birth recode, individual recode, sotred within that same directory. 

Now to have a quick look at what was actually downloaded using the download_survey function.

```{r Explore filenames, include=TRUE, message = FALSE, warning = FALSE}
str(downloads)
```

What we have is the file paths to where the files were saved to. Depending on the download option
this will either be the zip file, the extracted files from the zip, or the rds object created by unzipping
the zip file and rading in the approporiate dataset. 

A note on the rds object. These will be automatically reformatted to remove factors and simply return the raw character representations
of the data. This is carried out using \code{rdhs:::dta_factor_format} and \code{rdhs:::haven_factor_format}, that will read the stata and
spss dataset attributes and subsequently correct factors. This will also return a table detailing what the dataset codes were for that survey and
what their description is. This will be very helpful in the next section when we aim to find all the possible questions we may want to query
our surveys by. The rationale behind this was simply that the factor levels will often not be consistent between surveys, and by carrying this 
out in the backend when these datasets are read in then it saves the user from having to look up factor levels later on. If you do not wish to have
this information then change the `reformat` argument to `FALSE`.

A quick look at a demonstration of the reformat:

```{r Explore downloads, include=TRUE, message = FALSE, warning = FALSE}

# let's read one result in and look at that
downloaded_dataset <- readRDS(downloads[[1]][1])
str(downloaded_dataset,list.len = 10)

# have a look at the survey code descriptions
head(downloaded_dataset$Survey_Code_Descrptions)

# and let's have a look at how the factor reformat has been helpful
downloaded_dataset$Survey_Code_Descrptions[downloaded_dataset$Survey_Code_Descrptions$Code=="ML101",]
head(downloaded_dataset$Survey$ML101,20)

```
```{r Explore downloads 2, include=TRUE, message = FALSE, warning = FALSE}

# have a look at the survey code descriptions
head(downloaded_dataset$Survey_Code_Descrptions)

```

```{r Explore downloads3, include=TRUE, message = FALSE, warning = FALSE}

# and let's have a look at how the factor reformat has been helpful
downloaded_dataset$Survey_Code_Descrptions[downloaded_dataset$Survey_Code_Descrptions$Code=="ML101",]
head(downloaded_dataset$Survey$ML101,20)

```

## 6. Creating the list of survey questions required for your investigation




## 7. Using the list from step 6 to create your data frame of survey responses

## 8. Storing your pipeline from step 1 - 7 to increase reproducibility


## Summary and further thoughts

Hopefully the above tutorial has shown how the *rdhs* package can facilitate both querying the DHS
api and hopefully life some of the difficulty of interacting with the raw datasets. Any suggestions or 
comments/corrections/errors/ideas please let me know either in the issues or send me an email at 
"o.watson15@imperial.ac.uk".

---


